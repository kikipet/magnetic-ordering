{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "import torch_scatter\n",
    "\n",
    "import e3nn\n",
    "from e3nn import o3\n",
    "from data_helpers import DataPeriodicNeighbors\n",
    "from e3nn.nn.models.gate_points_2101 import Convolution, Network\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "import pymatgen as mg\n",
    "import pymatgen.io\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "import pymatgen.analysis.magnetism.analyzer as pg\n",
    "import numpy as np\n",
    "import pickle\n",
    "from mendeleev import element\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import io\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('magnetic_ordering_data.pt')\n",
    "\n",
    "run_name = (time.strftime(\"%y%m%d-%H%M\", time.localtime()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "params = {'len_embed_feat': 64,\n",
    "          'num_channel_irrep': 32,\n",
    "          'num_e3nn_layer': 2,\n",
    "          'max_radius': 5,\n",
    "          'num_basis': 10,\n",
    "          'adamw_lr': 0.005,\n",
    "          'adamw_wd': 0.03,\n",
    "          'radial_layers': 3\n",
    "          }\n",
    "\n",
    "# Used for debugging\n",
    "identification_tag = \"1:1:1.1 Relu wd:0.03 4 Linear\"\n",
    "cost_multiplier = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_element = 118\n",
    "atom_types_dim = 3*len_element\n",
    "embedding_dim = params['len_embed_feat']\n",
    "lmax = 1\n",
    "\n",
    "# I think the in and out irreps have to be scalars, but this is causing issues with no paths being created? like, from what I know about tensor products I'm *pretty sure* you need at least 1 path to add things over, but I'm not 100% sure\n",
    "\n",
    "# num_atom_types scalars (L=0) with even parity\n",
    "irreps_in = Irreps([(45, (0, 1))])\n",
    "irreps_hidden = Irreps([(64, (0, 1)), (64, (1, 1))])  # not sure - is this too large?\n",
    "irreps_out = Irreps([(3, (0, 1))])  # len_dos scalars (L=0) with even parity\n",
    "\n",
    "model_kwargs = {\n",
    "    \"irreps_in\": irreps_in,\n",
    "    \"irreps_hidden\": irreps_hidden,\n",
    "    \"irreps_out\": irreps_out,\n",
    "    \"irreps_node_attr\": '3x0e',  # not really sure, but I think it needs dim=3\n",
    "    # \"irreps_node_attr\": '0e',  # use this if I'm not inputting a z argument\n",
    "    \"irreps_edge_attr\": '1x1o',  # not really sure\n",
    "    \"layers\": params['num_e3nn_layer'],\n",
    "    \"max_radius\": params['max_radius'],\n",
    "    \"number_of_basis\": params['num_basis'],\n",
    "    \"radial_layers\": params['radial_layers'],\n",
    "    # for these last 3 I don't know what's normal\n",
    "    \"radial_neurons\": 35,\n",
    "    \"num_neighbors\": 35, # I think this is correct\n",
    "    \"num_nodes\": 35\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEmbeddingAndSumLastLayer(torch.nn.Module):\n",
    "    def __init__(self, atom_type_in, atom_type_out, model):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(atom_type_in, 128)\n",
    "        self.model = model\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(128, 96)\n",
    "        self.linear3 = torch.nn.Linear(96, 64)\n",
    "        self.linear4 = torch.nn.Linear(64, 45)\n",
    "        #self.linear5 = torch.nn.Linear(45, 32)\n",
    "        #self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, *args, batch=None, **kwargs):\n",
    "        print(args)\n",
    "        output = self.linear(x)\n",
    "        output = self.relu(output)\n",
    "        print(f\"Input: {x}\")\n",
    "        output = self.linear2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear3(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear4(output)\n",
    "        #output = self.linear5(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.model({'x': output, 'batch': batch, **kwargs})\n",
    "        if batch is None:\n",
    "            N = output.shape[0]\n",
    "            batch = output.new_ones(N)\n",
    "        # print(f'not-quite-output: {output}')\n",
    "        # output = torch_scatter.scatter_add(output, batch, dim=0)\n",
    "        print(f\"Output: {output}\")\n",
    "        #output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AtomEmbeddingAndSumLastLayer(\n",
    "    atom_types_dim, embedding_dim, Network(**model_kwargs))\n",
    "opt = torch.optim.AdamW(\n",
    "    model.parameters(), lr=params['adamw_lr'], weight_decay=params['adamw_wd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(data))\n",
    "np.random.shuffle(indices)\n",
    "index_tr, index_va, index_te = np.split(\n",
    "    indices, [int(.8 * len(indices)), int(.9 * len(indices))])\n",
    "\n",
    "assert set(index_tr).isdisjoint(set(index_te))\n",
    "assert set(index_tr).isdisjoint(set(index_va))\n",
    "assert set(index_te).isdisjoint(set(index_va))\n",
    "\n",
    "\n",
    "with open('loss.txt', 'a') as f:\n",
    "    f.write(f\"Iteration: {identification_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = torch_geometric.loader.DataLoader(\n",
    "    [data[i] for i in index_tr], batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = torch_geometric.loader.DataLoader(\n",
    "    [data[i] for i in index_va], batch_size=batch_size)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglinspace(rate, step, end=None):\n",
    "    t = 0\n",
    "    while end is None or t <= end:\n",
    "        yield t\n",
    "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    loss_cumulative = 0.\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for j, d in enumerate(dataloader):\n",
    "            d.to(device)\n",
    "            output = model(x=d.x, batch=d.batch, pos=d.pos, z=d.pos.new_ones((d.pos.shape[0], 3))) # CHANGED\n",
    "            if d.y.item() == 2:\n",
    "                loss = cost_multiplier*loss_fn(output, d.y).cpu()\n",
    "                print(\"Multiplied Loss Index \\n\")\n",
    "            elif d.y.item() == 0 or d.y.item() == 1:\n",
    "                loss = loss_fn(output, d.y).cpu()\n",
    "                print(\"Standard Loss Index \\n\")\n",
    "            else:\n",
    "                print(\"Lost datapoint \\n\")\n",
    "            loss_cumulative = loss_cumulative + loss.detach().item()\n",
    "    return loss_cumulative / len(dataloader)\n",
    "\n",
    "def train(model, optimizer, dataloader, dataloader_valid, max_iter=101, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "\n",
    "    checkpoint_generator = loglinspace(3.3, 5)\n",
    "    checkpoint = next(checkpoint_generator)\n",
    "    start_time = time.time()\n",
    "    dynamics = []\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        model.train()\n",
    "        loss_cumulative = 0.\n",
    "        for j, d in enumerate(dataloader):\n",
    "            d.to(device)\n",
    "            output = model(x=d.x, batch=d.batch, pos=d.pos, z=d.pos.new_ones((d.pos.shape[0], 3))) # CHANGED oh boy I hope this is right\n",
    "            loss = loss_fn(output, d.y).cpu()\n",
    "            print(f\"Iteration {step+1:4d}    batch {j+1:5d} / {len(dataloader):5d}   \" + f\"batch loss = {loss.data}\", end=\"\\r\", flush=True)\n",
    "            loss_cumulative = loss_cumulative + loss.detach().item()\n",
    "            optimizer.zero_grad()\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        wall = end_time - start_time\n",
    "\n",
    "        if step == checkpoint:\n",
    "            checkpoint = next(checkpoint_generator)\n",
    "            assert checkpoint > step\n",
    "\n",
    "            valid_avg_loss = evaluate(model, dataloader_valid, device)\n",
    "            train_avg_loss = evaluate(model, dataloader, device)\n",
    "\n",
    "            dynamics.append({\n",
    "                'step': step,\n",
    "                'wall': wall,\n",
    "                'batch': {'loss': loss.item(),},\n",
    "                'valid': {'loss': valid_avg_loss,},\n",
    "                'train': {'loss': train_avg_loss,},\n",
    "            })\n",
    "\n",
    "            yield {\n",
    "                'dynamics': dynamics,\n",
    "                'state': model.state_dict()\n",
    "            }\n",
    "\n",
    "            print(f\"Iteration {step+1:4d}    batch {j+1:5d} / {len(dataloader):5d}   \" +\n",
    "                  f\"train loss = {train_avg_loss:8.3f}   \" +\n",
    "                  f\"valid loss = {valid_avg_loss:8.3f}   \" +\n",
    "                  f\"elapsed time = {time.strftime('%H:%M:%S', time.gmtime(wall))}\")\n",
    "            with open('loss.txt', 'a') as f:\n",
    "                f.write(f\"train average loss: {str(train_avg_loss)} \\n\")\n",
    "                f.write(f\" validation average loss: {str(valid_avg_loss)} \\n\")\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Input: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Output: tensor([[0., 0., 0.]])\n",
      "tensor(1.0986)    batch     1 /  5151   batch loss = 1.0986122886681098\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2798237/496712852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2798237/991260059.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, dataloader_valid, max_iter, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/e3nn_new/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/e3nn_new/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "for results in train(model, opt, dataloader, dataloader_valid, device=device, max_iter=45):\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a76aca0647ecd8dec4c3fdc40dacc2d3e474d7a7a3cb408df2817eb18c7a4f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('e3nn_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
